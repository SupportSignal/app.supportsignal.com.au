# Story 6.5: LLM Model Management Upgrade

## Status
Approved

## Story
**As a** system administrator and developer,
**I want** to upgrade the default LLM model to a higher quality option and manage model selection per prompt,
**so that** AI-generated content has better quality while maintaining flexibility to choose cost-effective models for specific use cases.

## Acceptance Criteria

1. **Default Model Upgrade**: Change default LLM model from `gpt-4o-mini` to `gpt-5` (best quality)
2. **Environment Configuration**: Update `LLM_MODEL` environment variable to new default
3. **Schema Migration**: Populate all existing prompts with default model where `ai_model` is NULL
4. **Required Field**: Change `ai_prompts.ai_model` from optional to required in schema
5. **Model Dropdown**: Add model selection dropdown to prompt editor UI
6. **Model Display**: Show current model in prompt listing/management interface
7. **Bulk Model Assignment**: Select multiple prompts and assign a new model in bulk
8. **Validation**: Ensure all new prompts have a model selected (required field validation)

## Estimation & Planning

### Story Points
**3**

### Estimated Complexity
**Low-Medium**

Complexity drivers:
- Schema migration with data backfill
- Bulk selection UI implementation
- Model dropdown integration in existing UI
- Environment variable updates across environments

### Estimated Time
**1-2 days**

Breakdown:
- Environment update + default model change: 0.25 day
- Schema migration script: 0.25 day
- Make field required: 0.25 day
- Model dropdown UI: 0.5 day
- Model display in listing: 0.25 day
- Bulk assignment UI: 0.5 day
- Testing and validation: 0.5 day

### Risk Level
**Low**

**Risk Factors:**
- Breaking existing prompts if migration incomplete
- Model validation edge cases
- Environment variable deployment timing

**Mitigation Strategies:**
- **Migration First**: Populate all prompts before making field required
- **Validation**: Verify all prompts have model before schema change
- **Default Fallback**: Use environment default if model not specified
- **Staged Rollout**: Dev ‚Üí Production deployment sequence

## Tasks / Subtasks

### Phase 0: Environment Configuration Update (AC: 1, 2)

- [ ] **Task 0.1**: Update environment variable defaults
  - [ ] Update `~/.env-configs/app.supportsignal.com.au.env`:
    - Change `LLM_MODEL` from `openai/gpt-4o-mini` to `openai/gpt-5`
    - Document model upgrade reason (best quality for NDIS compliance)
    - Keep `LLM_FALLBACK_MODEL` as `openai/gpt-4o-mini` (cost-effective fallback)
  - [ ] Run `bun run sync-env --mode=local` to update local `.env.local`
  - [ ] Verify local environment has new default

- [ ] **Task 0.2**: Update code defaults
  - [ ] Update `apps/convex/lib/ai/questionGenerator.ts:102,110`:
    - Change fallback from `'openai/gpt-4o-mini'` to `'openai/gpt-5'`
  - [ ] Search codebase for other hardcoded model references
  - [ ] Update any other default model references found

- [ ] **Task 0.3**: Deploy environment changes
  - [ ] Deploy to development: `bun run sync-env --mode=deploy-dev`
  - [ ] Verify dev environment updated
  - [ ] Deploy to production: `bun run sync-env --mode=deploy-prod`
  - [ ] Verify production environment updated

### Phase 1: Schema Migration (AC: 3)

- [ ] **Task 1.1**: Create migration script
  - [ ] Create file: `apps/convex/migrations/populatePromptModels.ts`
  - [ ] Implement migration logic:
    ```typescript
    // For each prompt where ai_model is NULL/undefined:
    // 1. Get default model from config (getConfig().llm.defaultModel)
    // 2. Update prompt.ai_model = defaultModel
    // 3. Log update for audit trail
    ```
  - [ ] Add comprehensive logging
  - [ ] Add validation to verify all prompts updated

- [ ] **Task 1.2**: Test migration in development
  - [ ] Count prompts with NULL ai_model before migration
  - [ ] Run migration script
  - [ ] Verify all prompts now have ai_model populated
  - [ ] Check logs for any errors
  - [ ] Document migration results

- [ ] **Task 1.3**: Execute migration in production
  - [ ] Count production prompts with NULL ai_model
  - [ ] Run migration on production database
  - [ ] Verify all production prompts have ai_model
  - [ ] Document production migration completion

### Phase 2: Schema Update (AC: 4)

- [ ] **Task 2.1**: Make ai_model field required
  - [ ] Update `apps/convex/schema.ts:435`:
    - Change from: `ai_model: v.optional(v.string())`
    - Change to: `ai_model: v.string()` (required)
  - [ ] Deploy schema to development
  - [ ] Verify schema deployed successfully

- [ ] **Task 2.2**: Update prompt creation logic
  - [ ] Locate prompt creation function(s)
  - [ ] Ensure ai_model is always provided during creation
  - [ ] Use config default if not specified: `getConfig().llm.defaultModel`
  - [ ] Add validation to reject prompts without model

- [ ] **Task 2.3**: Deploy schema to production
  - [ ] Deploy schema changes to production
  - [ ] Verify production schema updated
  - [ ] Monitor for any validation errors

### Phase 3: Model Dropdown UI (AC: 5, 8)

- [ ] **Task 3.1**: Create model selection component
  - [ ] Create shared component: `apps/web/components/admin/model-selector.tsx`
  - [ ] Component features:
    - Dropdown with all supported models from `SUPPORTED_MODELS`
    - Display model name, provider, cost estimate
    - Visual indicator for recommended models
    - Search/filter capability for large model list
  - [ ] Use ShadCN Select component pattern
  - [ ] Add validation: required field, must be valid model ID

- [ ] **Task 3.2**: Integrate dropdown into prompt editor
  - [ ] Locate prompt editor/creation UI
  - [ ] Add model selection dropdown (after prompt name field)
  - [ ] Default to environment default model
  - [ ] Add "Model" field label with cost estimate tooltip
  - [ ] Show validation error if model not selected
  - [ ] Update form submission to include model

- [ ] **Task 3.3**: Add model info display
  - [ ] Show selected model's cost per 1K tokens
  - [ ] Display context window size
  - [ ] Add provider badge (OpenAI, Anthropic, etc.)
  - [ ] Visual feedback when changing models

### Phase 4: Model Display in Listing (AC: 6)

- [ ] **Task 4.1**: Update prompt listing query
  - [ ] Ensure prompt listing includes `ai_model` field
  - [ ] Add model metadata to response (name, provider, cost)
  - [ ] Sort/filter capability by model (optional enhancement)

- [ ] **Task 4.2**: Display model in prompt listing
  - [ ] Locate prompt listing component
  - [ ] Add "Model" column to table (after Version column)
  - [ ] Display model name with provider badge
  - [ ] Show cost estimate on hover/tooltip
  - [ ] Use consistent badge styling across UI

### Phase 5: Bulk Model Assignment (AC: 7)

- [ ] **Task 5.1**: Add bulk selection UI
  - [ ] Add checkboxes to prompt listing table
  - [ ] "Select All" / "Select None" controls
  - [ ] Display count of selected prompts
  - [ ] Visual feedback for selection state

- [ ] **Task 5.2**: Create bulk assignment mutation
  - [ ] Create backend function: `promptManager.bulkUpdateModel(sessionToken, promptIds, newModel)`
  - [ ] System admin permission check
  - [ ] Validate all prompt IDs exist
  - [ ] Update all selected prompts with new model
  - [ ] Return summary: successful updates, errors

- [ ] **Task 5.3**: Build bulk assignment UI
  - [ ] "Bulk Actions" dropdown appears when prompts selected
  - [ ] "Assign Model" action opens dialog
  - [ ] Dialog shows:
    - Count of selected prompts
    - Model selection dropdown (same component as editor)
    - Cost impact estimate (optional)
    - Confirmation button
  - [ ] Execute bulk update on confirmation
  - [ ] Toast notification with results
  - [ ] Clear selection and refresh listing

### Phase 6: Testing & Validation (All AC)

- [ ] **Task 6.1**: Test environment configuration
  - [ ] Verify new default model in both dev and production
  - [ ] Test prompt creation uses new default
  - [ ] Verify fallback model still works
  - [ ] Check config validation passes

- [ ] **Task 6.2**: Test migration completeness
  - [ ] Verify no prompts have NULL ai_model in dev
  - [ ] Verify no prompts have NULL ai_model in production
  - [ ] Test creating new prompts (should require model)
  - [ ] Verify migration logs accurate

- [ ] **Task 6.3**: Test model selection UI
  - [ ] Test model dropdown shows all supported models
  - [ ] Test default model auto-selected
  - [ ] Test validation prevents empty model
  - [ ] Test model change updates correctly
  - [ ] Verify cost estimates display accurately

- [ ] **Task 6.4**: Test bulk assignment
  - [ ] Test selecting multiple prompts
  - [ ] Test bulk model assignment
  - [ ] Verify all selected prompts updated
  - [ ] Test partial selection
  - [ ] Verify error handling if update fails

- [ ] **Task 6.5**: Test model display
  - [ ] Verify model shown in listing
  - [ ] Test model badges display correctly
  - [ ] Verify cost estimates accurate
  - [ ] Test tooltip/hover states

## Documentation Impact Assessment

### Architectural Patterns
- **Default Configuration Pattern**: Environment-based defaults for system-wide settings (ESTABLISHED)
- **Required Field Migration Pattern**: Make optional field required after backfilling data (ESTABLISHED)
- **Bulk Update Pattern**: Multi-select and batch operations on records (NEW)

### Documentation Updates Required
- `docs/patterns/bulk-update-ui-pattern.md` - Document multi-select and bulk operations (CREATE)
- `docs/examples/bulk-operations/` - Example bulk assignment implementation (CREATE if significant)
- `docs/configuration/environment-variables.md` - Update LLM model configuration (UPDATE)

### Knowledge Capture Opportunities
- **Model migration strategy**: Safe pattern for making optional fields required
- **Bulk operations UI**: Reusable pattern for bulk actions on table data
- **Model cost visibility**: Displaying cost estimates to inform selection

### Examples to Create
- Bulk selection component with checkboxes
- Model dropdown with cost estimates
- Migration script with validation

## Dev Notes

### Current Model Configuration

**Environment Variables** [Source: apps/convex/lib/config.ts]
- `LLM_MODEL`: Default model for AI operations
- `LLM_FALLBACK_MODEL`: Fallback when primary fails
- Current default: `openai/gpt-4o-mini` ($0.15/M tokens)
- **New default**: `openai/gpt-5` ($1.25/M tokens) - 8.3x cost increase, best quality

**Supported Models** [Source: apps/convex/lib/config.ts:126-194]
```typescript
export const SUPPORTED_MODELS: ModelInfo[] = [
  // Cost-effective (recommended)
  { id: 'openai/gpt-5-nano', costPer1kTokens: 0.00005, recommended: true },
  { id: 'openai/gpt-4o-mini', costPer1kTokens: 0.00015, recommended: true },
  { id: 'anthropic/claude-3-haiku', costPer1kTokens: 0.00025, recommended: true },

  // Mid-range
  { id: 'openai/gpt-5-mini', costPer1kTokens: 0.00025 },
  { id: 'anthropic/claude-3-sonnet', costPer1kTokens: 0.003 },
  { id: 'openai/gpt-4o', costPer1kTokens: 0.005 }, // ‚Üê NEW DEFAULT

  // Premium
  { id: 'openai/gpt-5-chat', costPer1kTokens: 0.00125 },
  { id: 'openai/gpt-5', costPer1kTokens: 0.00125 },
];
```

### AI Prompts Schema (Current)

[Source: apps/convex/schema.ts:415-444]

```typescript
ai_prompts: defineTable({
  // Identity
  prompt_name: v.string(),
  prompt_version: v.string(),

  // Content
  prompt_template: v.string(),
  description: v.optional(v.string()),
  input_schema: v.optional(v.string()),
  output_schema: v.optional(v.string()),

  // Developer scoping
  scope: v.optional(v.union(v.literal("production"), v.literal("developer"))),
  developer_session_id: v.optional(v.string()),
  parent_prompt_id: v.optional(v.id("ai_prompts")),
  expires_at: v.optional(v.number()),

  // Usage Context
  workflow_step: v.optional(v.string()),
  subsystem: v.optional(v.string()),
  ai_model: v.optional(v.string()), // ‚ö†Ô∏è CHANGING: Make required
  max_tokens: v.optional(v.number()),
  temperature: v.optional(v.number()),

  // Versioning
  is_active: v.optional(v.boolean()),
  created_at: v.number(),
  created_by: v.optional(v.id("users")),
  replaced_at: v.optional(v.number()),
  replaced_by: v.optional(v.string()),
})
```

**Schema Changes:**
- Change `ai_model` from `v.optional(v.string())` to `v.string()` (required)

### Migration Strategy

**Safe Migration Pattern** [Source: Story 7.4 lessons]
1. **Step 1**: Populate all NULL ai_model values with default
2. **Step 2**: Verify 100% of prompts have ai_model populated
3. **Step 3**: Make field required in schema (prevents new NULL values)
4. **Step 4**: Deploy schema change

**Migration Script Pattern:**
```typescript
// apps/convex/migrations/populatePromptModels.ts
import { mutation } from '../_generated/server';
import { getConfig } from '../lib/config';

export default mutation({
  args: {},
  handler: async (ctx) => {
    const config = getConfig();
    const defaultModel = config.llm.defaultModel; // Will be 'openai/gpt-5'

    // Get all prompts
    const prompts = await ctx.db.query("ai_prompts").collect();

    let updatedCount = 0;
    for (const prompt of prompts) {
      if (!prompt.ai_model) {
        await ctx.db.patch(prompt._id, {
          ai_model: defaultModel,
        });
        updatedCount++;
        console.log(`Updated prompt ${prompt.prompt_name} (${prompt._id}) with model ${defaultModel}`);
      }
    }

    console.log(`Migration complete: ${updatedCount} prompts updated with ${defaultModel}`);
    return {
      totalPrompts: prompts.length,
      updatedCount,
      defaultModel,
    };
  },
});
```

### Bulk Update Pattern

**Backend Mutation:**
```typescript
// apps/convex/promptManager.ts
export const bulkUpdateModel = mutation({
  args: {
    sessionToken: v.string(),
    promptIds: v.array(v.id("ai_prompts")),
    newModel: v.string(),
  },
  handler: async (ctx, args) => {
    // System admin permission check
    await requirePermission(ctx, args.sessionToken, PERMISSIONS.MANAGE_SYSTEM);

    // Validate model ID
    const modelInfo = getModelInfo(args.newModel);
    if (!modelInfo) {
      throw new ConvexError(`Invalid model: ${args.newModel}`);
    }

    // Update all prompts
    const results = {
      successful: 0,
      failed: 0,
      errors: [] as string[],
    };

    for (const promptId of args.promptIds) {
      try {
        const prompt = await ctx.db.get(promptId);
        if (!prompt) {
          results.failed++;
          results.errors.push(`Prompt ${promptId} not found`);
          continue;
        }

        await ctx.db.patch(promptId, { ai_model: args.newModel });
        results.successful++;

        console.log(`Bulk update: ${prompt.prompt_name} ‚Üí ${args.newModel}`);
      } catch (error) {
        results.failed++;
        results.errors.push(`Error updating ${promptId}: ${error.message}`);
      }
    }

    console.log(`Bulk update complete: ${results.successful} updated, ${results.failed} failed`);
    return results;
  },
});
```

**Frontend UI Pattern:**
```typescript
// Bulk selection state
const [selectedPrompts, setSelectedPrompts] = useState<Id<"ai_prompts">[]>([]);

// Bulk actions dropdown
{selectedPrompts.length > 0 && (
  <div className="flex items-center gap-2">
    <span className="text-sm text-muted-foreground">
      {selectedPrompts.length} selected
    </span>
    <DropdownMenu>
      <DropdownMenuTrigger asChild>
        <Button variant="outline">Bulk Actions</Button>
      </DropdownMenuTrigger>
      <DropdownMenuContent>
        <DropdownMenuItem onClick={() => setShowBulkModelDialog(true)}>
          Assign Model
        </DropdownMenuItem>
      </DropdownMenuContent>
    </DropdownMenu>
  </div>
)}
```

### Model Selection Component

**Reusable Component Pattern:**
```typescript
// apps/web/components/admin/model-selector.tsx
import { SUPPORTED_MODELS } from '@/convex/lib/config';

interface ModelSelectorProps {
  value: string;
  onChange: (model: string) => void;
  showCost?: boolean;
  error?: string;
}

export function ModelSelector({ value, onChange, showCost = true, error }: ModelSelectorProps) {
  const selectedModel = SUPPORTED_MODELS.find(m => m.id === value);

  return (
    <div className="space-y-2">
      <Label>AI Model</Label>
      <Select value={value} onValueChange={(val) => val && onChange(val)}>
        <SelectTrigger>
          <SelectValue placeholder="Select model" />
        </SelectTrigger>
        <SelectContent>
          {SUPPORTED_MODELS.map(model => (
            <SelectItem key={model.id} value={model.id}>
              <div className="flex items-center justify-between w-full">
                <span>{model.name}</span>
                <div className="flex items-center gap-2 text-xs">
                  <Badge variant="outline">{model.provider}</Badge>
                  {showCost && (
                    <span className="text-muted-foreground">
                      ${model.costPer1kTokens.toFixed(5)}/1K
                    </span>
                  )}
                  {model.recommended && <Badge variant="secondary">Recommended</Badge>}
                </div>
              </div>
            </SelectItem>
          ))}
        </SelectContent>
      </Select>
      {showCost && selectedModel && (
        <p className="text-xs text-muted-foreground">
          Cost: ${selectedModel.costPer1kTokens.toFixed(5)} per 1,000 tokens
          ({selectedModel.contextWindow.toLocaleString()} token context)
        </p>
      )}
      {error && <p className="text-xs text-destructive">{error}</p>}
    </div>
  );
}
```

### File Locations

**Backend Files:**
- `apps/convex/schema.ts` - Update ai_prompts schema
- `apps/convex/migrations/populatePromptModels.ts` - Migration script (NEW)
- `apps/convex/promptManager.ts` - Add bulkUpdateModel mutation
- `apps/convex/lib/config.ts` - Already has model definitions
- `apps/convex/lib/ai/questionGenerator.ts` - Update default model fallback
- `~/.env-configs/app.supportsignal.com.au.env` - Update LLM_MODEL

**Frontend Files:**
- `apps/web/components/admin/model-selector.tsx` - Model dropdown component (NEW)
- Prompt editor component - Add model selection
- Prompt listing component - Add model display and bulk actions

### Cost Impact Analysis

**Current vs New Default:**
- **Current**: `openai/gpt-4o-mini` - $0.15 per 1M tokens
- **New**: `openai/gpt-5` - $1.25 per 1M tokens
- **Cost Increase**: 8.3x more expensive

**Example Costs (Per Request):**
- 10K tokens (medium prompt): $0.0015 ‚Üí $0.0125 (+$0.011 or 1.1¬¢)
- 100K tokens (large document): $0.015 ‚Üí $0.125 (+$0.11 or 11¬¢)
- 1M tokens (heavy usage): $0.15 ‚Üí $1.25 (+$1.10)

**Monthly Cost Estimates** (Based on Current Prompt Usage):

| Scenario | Incidents/Month | Prompts/Incident | Current Cost (gpt-4o-mini) | New Cost (gpt-5) | Monthly Increase |
|----------|----------------|------------------|---------------------------|-----------------|------------------|
| **Low Volume** | 20 | 8 | $0.12 | $1.00 | +$0.88 |
| **Medium Volume** | 100 | 8 | $0.60 | $5.00 | +$4.40 |
| **High Volume** | 500 | 8 | $3.00 | $25.00 | +$22.00 |

**Per-Prompt Type Analysis:**

| Prompt Type | Avg Tokens | Requests/Month* | Current Cost | New Cost | Increase |
|-------------|-----------|----------------|--------------|----------|----------|
| Question Generation (4 prompts) | 2000 | 400 | $0.12 | $1.00 | +$0.88 |
| Narrative Enhancement (4 prompts) | 4000 | 400 | $0.24 | $2.00 | +$1.76 |
| Mock Data (1 prompt) | 2000 | 10 | $0.003 | $0.025 | +$0.022 |
| **Total** | - | 810 | **$0.36** | **$3.00** | **+$2.64** |

\* Based on 100 incidents/month, 4 phases each

**Quality vs Cost Trade-Off:**
- **Cost increase**: 733% more expensive
- **Quality improvement**: Best-in-class for writing, reasoning, NDIS compliance
- **Risk mitigation**: Superior quality reduces regulatory issues, rework, complaints
- **User requirement**: "Most expensive model for now, the best quality"

**Recommendation**:
- ‚úÖ Use `gpt-5` as default for best quality
- ‚úÖ Keep `gpt-4o-mini` as fallback for reliability
- ‚ö†Ô∏è Monitor actual costs after deployment
- üí° Future optimization: Consider tiered approach (see Optional Optimization below)

### Optional Cost Optimization Strategy (Future Consideration)

**Tiered Model Assignment** - Not implementing in this story, but documented for future reference:

| Prompt Category | Recommended Model | Reason | Monthly Cost* |
|----------------|------------------|--------|--------------|
| **Narrative Enhancement** (4) | `gpt-5` ($1.25/M) | Critical quality, legal compliance | $2.00 |
| **Question Generation** (4) | `gpt-5-mini` ($0.25/M) | Good reasoning, cost-effective | $0.40 |
| **Mock Data** (1) | `gpt-5-nano` ($0.05/M) | Dev only, non-critical | $0.02 |
| **Total (Optimized)** | - | - | **$2.42** |

**Potential Savings**: $2.64/month (48% reduction) while maintaining quality where critical

**Implementation Note**: Start with single default (`gpt-5`), optimize later based on actual usage patterns.

### GPT-5 vs Other Models Comparison

| Model | Input Cost/M | Performance vs GPT-5 | Best For | Context Window |
|-------|--------------|---------------------|----------|----------------|
| **gpt-5** (full) | $1.25 | 100% | Critical writing, legal docs, max quality | 400K |
| **gpt-5-mini** | $0.25 | 92% | Production default, best ROI | 400K |
| **gpt-4o** | $5.00 | 95% (GPT-4 gen) | Not recommended (more $ than gpt-5) | 128K |
| **gpt-4o-mini** | $0.15 | 85-90% | Fallback, cost-conscious | 128K |
| **gpt-5-nano** | $0.05 | 75-80% | Simple tasks, high volume | 400K |

**Key GPT-5 Advantages** [Source: docs/analysis/gpt-5-model-recommendations.md]
- 55-90% cheaper than GPT-4o for similar tasks
- 90% caching discount on repeated inputs
- 19-24% higher in reasoning tests
- 2x context window of GPT-4o (272K vs 128K)
- Superior coding (75% vs 31%), math, writing
- Best-in-class for writing use cases

**Rationale for gpt-5 Default:**
1. **Quality**: Best writing quality (NDIS narratives are critical legal documents)
2. **Cost**: More affordable than gpt-4o ($1.25 vs $5.00/M)
3. **Context**: 400K tokens handles long narratives with Q&A
4. **Performance**: Faster response times than GPT-4o
5. **Future-proof**: Latest model with ongoing improvements

### Testing

[Source: docs/testing/technical/test-strategy-and-standards.md]

**Test Location:** Centralized in `tests/` directory

**Test Files to Create:**
- `tests/convex/migrations/populate-prompt-models.test.ts` - Migration script tests
- `tests/convex/promptManager/bulk-update-model.test.ts` - Bulk update tests
- `tests/web/integration/model-selection.test.ts` - Model selector UI tests

**Key Test Scenarios:**
- Migration populates all NULL ai_model values
- Required field validation rejects prompts without model
- Model dropdown shows all supported models
- Bulk assignment updates multiple prompts
- Model display shows correct info in listing
- Cost estimates calculate accurately
- Invalid model IDs rejected

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-22 | 1.0 | Story created - LLM model management upgrade | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
_To be populated by developer agent_

### Debug Log References
_To be populated by developer agent during implementation_

### Completion Notes List
_To be populated by developer agent after implementation_

### File List
_To be populated by developer agent after implementation_

## Bugs Found During Story Acceptance Testing (SAT)
_To be populated during SAT execution_

## QA Results

### Pattern Compliance Review
_To be populated by QA agent after implementation_

### Knowledge Capture Reference
_To be populated by QA agent after implementation - Must reference knowledge base files per KDD process_

### Velocity Data
_To be populated by QA agent after implementation_
