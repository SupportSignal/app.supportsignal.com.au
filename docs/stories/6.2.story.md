# Story 6.2: Phase-Specific Question Generation Prompts

## Status
Done

## Story
**As a** system administrator,  
**I want** four specialized prompt templates for each incident phase instead of one generic template that uses phase as a variable,  
**so that** I can generate higher-quality, phase-specific clarification questions that focus on the most relevant investigation areas for each incident phase.

## Acceptance Criteria

1. **Four Phase-Specific Prompts**: Create specialized prompts for each incident phase:
   - `"generate_clarification_questions_before_event"`
   - `"generate_clarification_questions_during_event"`
   - `"generate_clarification_questions_end_event"`
   - `"generate_clarification_questions_post_event"`

2. **Dynamic Prompt Selection**: Update questionGenerator.ts to select prompt based on `args.phase`

3. **Hardcoded Phase Context**: Remove generic `{{phase}}` variable, embed phase context in each prompt

4. **Maintain Existing API**: No breaking changes to existing question generation interface

5. **Database Migration**: Add new prompts to DEFAULT_PROMPTS and seed in database

6. **Testing**: Verify each phase generates appropriate, phase-specific questions

7. **Documentation**: Update prompt management documentation with new phase-specific approach

## Estimation & Planning

### Story Points
5

### Estimated Complexity  
Medium

### Estimated Time
2-3 days

### Risk Level
Low

## Tasks / Subtasks

- [x] **Task 6.2.1: Create Four Phase-Specific Prompt Templates in Code** (AC: 1, 3, 5)
  - [x] Subtask 6.2.1.1: Design before_event prompt focusing on antecedents and environmental factors
  - [x] Subtask 6.2.1.2: Design during_event prompt focusing on interventions and safety measures
  - [x] Subtask 6.2.1.3: Design end_event prompt focusing on resolution strategies and outcomes
  - [x] Subtask 6.2.1.4: Design post_event prompt focusing on follow-up care and lessons learned
  - [x] Subtask 6.2.1.5: Add all four prompts to DEFAULT_PROMPTS array in promptManager.ts (lines 210+)

- [ ] **Task 6.2.2: Update Question Generator Logic** (AC: 2, 4)
  - [ ] Subtask 6.2.2.1: Modify questionGenerator.ts to dynamically select prompt based on phase
  - [ ] Subtask 6.2.2.2: Remove {{phase}} variable from template variable mapping
  - [ ] Subtask 6.2.2.3: Update template interpolation to use phase-specific prompts
  - [ ] Subtask 6.2.2.4: Maintain backward compatibility during transition

- [ ] **Task 6.2.3: Import File-Based Prompts to Database** (AC: 5)
  - [ ] Subtask 6.2.3.1: Run existing seedPromptTemplates mutation to import new DEFAULT_PROMPTS to database
  - [ ] Subtask 6.2.3.2: Test seeding process imports all four new phase-specific prompts
  - [ ] Subtask 6.2.3.3: Verify existing generic prompt can coexist during transition period
  - [ ] Subtask 6.2.3.4: Document file-to-database workflow for prompt management

- [ ] **Task 6.2.4: Testing and Validation** (AC: 6)
  - [ ] Subtask 6.2.4.1: Test each phase generates appropriate questions
  - [ ] Subtask 6.2.4.2: Verify no breaking changes to existing API
  - [ ] Subtask 6.2.4.3: Update existing test suites to work with new prompts
  - [ ] Subtask 6.2.4.4: Add phase-specific prompt testing

## Documentation Impact Assessment

This story establishes optimized AI prompt patterns for phase-specific question generation that will improve AI response quality:

**Architectural Patterns to Establish:**
- Phase-specific prompt template selection patterns
- Dynamic prompt selection based on context parameters
- Hardcoded context optimization vs. variable-driven approaches
- Prompt template specialization strategies

**Documentation Updates Needed:**
- Prompt management documentation with phase-specific approach
- Question generator architecture documentation
- AI prompt optimization patterns
- Template specialization best practices

**Knowledge Capture:**
- Phase-specific prompt effectiveness comparison
- Variable-driven vs hardcoded context trade-offs
- AI prompt specialization strategies for better results
- Template migration patterns for backward compatibility

## Dev Notes

### Previous Story Insights
From Story 6.1 (Core AI Prompt Management Foundation):
- **Prompt Template System**: Complete system with DEFAULT_PROMPTS, seeding, and template resolution
- **Variable Interpolation**: {{variable}} syntax with validation and substitution engine
- **Template Resolution Caching**: 5-minute TTL caching system for performance
- **Database Schema**: ai_prompts table with prompt_name, prompt_template, subsystem fields
- **Authentication Pattern**: sessionToken + requirePermission() for system admin access

### Current System Analysis [Source: apps/convex/lib/ai/questionGenerator.ts]

**Current Problem**:
- Single generic prompt `"generate_clarification_questions"` handles all phases
- Phase passed as `{{phase}}` variable requiring AI to interpret context
- Line 197-198: `await ctx.runQuery(internal.promptManager.getActivePrompt, { prompt_name: "generate_clarification_questions" })`
- Suboptimal because each phase requires different investigation focus areas

**Current Variable System**:
```typescript
// Template variables currently used (lines 56-65)
participantName: data.participant_name,
reporterName: data.reporter_name,
location: data.location,
eventDateTime: data.event_date_time,
phase: data.phase,  // <- This will be removed
narrativeText: data.narrative_content,
```

### Data Models [Source: apps/convex/schema.ts, Story 6.1]

**Existing AI Prompts Schema** (already implemented):
```typescript
ai_prompts: defineTable({
  prompt_name: v.string(),
  prompt_template: v.string(),
  subsystem: v.string(),
  description: v.string(),
  workflow_step: v.string(),
  // ... other fields
})
  .index("by_name", ["prompt_name"])
  .index("by_subsystem", ["subsystem"])
```

**File-Based Prompt Architecture**:
- **Source of Truth**: Prompts defined in `DEFAULT_PROMPTS` array in promptManager.ts (lines 210+)
- **Runtime Storage**: Database stores imported prompts for performance
- **Import Process**: `seedPromptTemplates` mutation imports file-based prompts to database
- **Development Workflow**: Modify code → run seeding → database updated

**New Prompt Names** (to be added to DEFAULT_PROMPTS):
- `"generate_clarification_questions_before_event"`
- `"generate_clarification_questions_during_event"`
- `"generate_clarification_questions_end_event"`
- `"generate_clarification_questions_post_event"`

### API Specifications

**Required Changes to Existing Functions**:

#### apps/convex/lib/ai/questionGenerator.ts:
```typescript
// Line 197 - Update from:
const prompt = await ctx.runQuery(internal.promptManager.getActivePrompt, {
  prompt_name: "generate_clarification_questions",
  subsystem: "incidents",
});

// To:
const promptName = `generate_clarification_questions_${args.phase}`;
const prompt = await ctx.runQuery(internal.promptManager.getActivePrompt, {
  prompt_name: promptName,
  subsystem: "incidents",
});
```

#### apps/convex/promptManager.ts DEFAULT_PROMPTS (lines 210+):
```typescript
// Add four new prompt objects to existing DEFAULT_PROMPTS array
// Each with specialized prompt_template optimized for specific phase context
// Use existing seedPromptTemplates mutation to import to database
const NEW_PHASE_PROMPTS = [
  {
    prompt_name: "generate_clarification_questions_before_event",
    prompt_template: "...", // Phase-specific template
    description: "Generate before-event focused clarification questions",
    workflow_step: "clarification_questions",
    subsystem: "incidents",
  },
  // ... three more phase-specific prompts
];
```

#### Import Process:
```typescript
// Use existing seeding function to import file-based prompts
await ctx.runMutation(api.promptManager.seedPromptTemplates, {
  sessionToken: "..." // System admin token
});
```

### Phase-Specific Optimizations

**Before Event Prompt Focus**:
- Antecedent conditions and environmental factors
- Participant state and behavior patterns
- Support strategies in place
- Environmental triggers or stressors

**During Event Prompt Focus**:
- Specific staff actions and interventions attempted
- Participant responses to different approaches
- Safety measures implemented
- Escalation or de-escalation techniques used

**End Event Prompt Focus**:
- Resolution strategies that were effective
- De-escalation techniques that worked
- Immediate safety outcomes
- Transition back to normal routine

**Post Event Prompt Focus**:
- Immediate support provided to participant
- Follow-up care and monitoring
- Support plan modifications needed
- Lessons learned for future prevention

### File Locations [Source: docs/architecture/source-tree.md]

**Backend Implementation**:
- `apps/convex/promptManager.ts` - Add four new prompt objects to DEFAULT_PROMPTS array (lines 210+)
- `apps/convex/lib/ai/questionGenerator.ts` - Update prompt selection logic (line 197)
- No new files required - leveraging existing file-based prompt system with database import

**Testing Implementation** [Source: testing standards]:
- `tests/convex/ai/question-generator-integration.test.ts` - Update for new prompts
- `tests/convex/prompt-templates/` - Add phase-specific prompt testing
- `tests/integration/` - Update clarification workflow tests

### Technical Constraints

**Backward Compatibility Requirements**:
- Existing `generateQuestionsForPhase` API must remain unchanged
- No breaking changes to calling code in clarification workflow
- Transition period where both old and new prompts can coexist

**Performance Requirements** [Source: Story 6.1]:
- Prompt resolution: <500ms for template selection and variable substitution
- Leverage existing 5-minute TTL caching system
- No additional database queries per phase-specific selection

**Variable System Updates**:
- Remove `{{phase}}` from template variables mapping (line 63)
- Keep all other variables: participantName, reporterName, location, eventDateTime, narrativeText
- Each phase-specific prompt embeds phase context directly in template

### Testing Standards [Source: docs/testing/technical/test-strategy-and-standards.md]

**Test File Locations**:
- Unit tests: `tests/convex/lib/ai/` - Question generator logic testing
- Integration tests: `tests/convex/ai/` - End-to-end clarification question generation
- Prompt tests: `tests/convex/prompt-templates/` - Phase-specific prompt validation

**Key Testing Scenarios**:
- Each phase generates appropriate, phase-specific questions
- No API breaking changes for existing callers
- Template resolution works correctly for all four phases
- Variable interpolation works without {{phase}} variable
- Backward compatibility during transition period

**Coverage Requirements**:
- Unit tests: 85%+ coverage for updated question generator logic
- Integration tests: 100% coverage of all four phase-specific prompts
- API compatibility tests: Verify no breaking changes

### Pattern Validation

**Repository Pattern** [Source: docs/architecture/coding-standards.md]:
- All prompt access through existing promptManager query functions
- No direct database access from question generator
- Centralized prompt resolution service (already established)

**TypeScript Requirements** [Source: docs/architecture/coding-standards.md]:
- Strict mode compliance maintained
- No `any` type usage in updated code
- Type-safe prompt name generation: `generate_clarification_questions_${phase}`

**File-Based Prompt Management Pattern** (Existing):
- All prompts defined in code as DEFAULT_PROMPTS array (source of truth)
- Database serves as runtime cache imported from code via seedPromptTemplates
- Development workflow: modify code → run seeding function → database updated
- Consistent with existing Story 6.1 architecture

**Template Specialization Pattern** (New):
- Establish pattern for specialized prompts vs. generic variable-driven prompts
- Document when to use hardcoded context vs. template variables
- Create reusable pattern for other AI operations requiring phase-specific behavior

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|---------|
| 2025-01-06 | 1.0 | Initial story creation for phase-specific question generation prompts | Bob (Scrum Master) |
| 2025-01-06 | 1.1 | Updated to correctly reflect file-based prompt architecture with import process | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
Claude Sonnet 4 (claude-sonnet-4-20250514)

### Debug Log References  
*To be filled by Dev Agent during implementation*

### Completion Notes List
*To be filled by Dev Agent during implementation*

### File List
*To be filled by Dev Agent during implementation*

## QA Results

*This section will be populated by QA Agent after story completion*

### Pattern Compliance Review
*To be filled by QA agent*

### Knowledge Capture
*To be filled by QA agent*  

### Velocity Data
*To be filled by QA agent*