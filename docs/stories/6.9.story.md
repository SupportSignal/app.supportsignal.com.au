# Story 6.9: Adaptive Token Management with Self-Healing

## Status

Approved

## Story

**As a** System Administrator,
**I want** the AI prompt system to automatically detect token truncation and adaptively increase token limits with database persistence and UI visibility,
**so that** all AI workflows never fail silently due to insufficient token limits, and the system self-heals without manual intervention.

## Acceptance Criteria

### Backend - Truncation Detection & Response Extraction (aiService.ts)

1. **Extract `finish_reason` from OpenRouter API response**
   - [ ] Modify `callOpenRouter` function to capture `finish_reason` field from `data.choices[0].finish_reason`
   - [ ] Return `finishReason` in response object alongside `content` and `usage`
   - [ ] Support all finish_reason values: `"stop"` (normal), `"length"` (truncated), `"content_filter"` (policy violation)

2. **Enable Truncation Detection**
   - [ ] Calling functions can check `response.finishReason === "length"` to detect truncation
   - [ ] Log truncation events with correlation ID for debugging

### Backend - Adaptive Retry with Token Escalation (aiClarification.ts)

3. **Replace Fixed Retry with Adaptive Token Escalation**
   - [ ] Create `retryWithAdaptiveTokens` function replacing current `retryWithBackoff`
   - [ ] **Escalation Pattern**: Start with baseline `max_tokens`, increase by 500 on truncation
   - [ ] **Maximum Escalations**: 3-4 attempts (baseline → +500 → +500 → +500)
   - [ ] **Token Cap**: Enforce configurable maximum (default 10,000 tokens via environment variable)
   - [ ] **Early Success Exit**: Stop escalating if `finishReason === "stop"` (normal completion)

4. **Truncation-Triggered Escalation Logic**
   - [ ] On `finishReason === "length"`: Increase `max_tokens` by 500 and retry
   - [ ] On JSON parse errors: Treat as potential truncation and escalate
   - [ ] On normal completion (`finish_reason === "stop"`): Exit successfully without further retries
   - [ ] On max escalations reached: Throw clear error with troubleshooting guidance

5. **Comprehensive Logging**
   - [ ] Log each escalation attempt with: current `max_tokens`, escalation increment, attempt number
   - [ ] Log final success with: total attempts, final `max_tokens` used, baseline comparison
   - [ ] Include correlation ID in all logs for end-to-end tracing

### Backend - Self-Healing Database Updates (promptManager.ts)

6. **Create `updatePromptTokenLimit` Mutation**
   - [ ] Accept parameters: `prompt_name`, `new_max_tokens`, `baseline_max_tokens`, `adjustment_reason`
   - [ ] Update `ai_prompts` table with new `max_tokens` value
   - [ ] Populate new schema fields: `baseline_max_tokens`, `adjusted_at`, `adjustment_reason`
   - [ ] Log update with correlation ID for audit trail

7. **Automatic Database Update on Successful Escalation**
   - [ ] After successful retry with increased tokens: Call `updatePromptTokenLimit` mutation
   - [ ] Pass: prompt name, final successful `max_tokens`, original baseline, auto-generated reason
   - [ ] Reason format: `"Auto-increased from {baseline} to {new_value} after {attempts} escalation attempts on {timestamp}"`
   - [ ] Update happens asynchronously (non-blocking to user response)

### Database Schema Changes (schema.ts)

8. **Add New Fields to `ai_prompts` Table**
   - [ ] `baseline_max_tokens: v.optional(v.number())` - Original coded template value (e.g., 2000)
   - [ ] `adjusted_at: v.optional(v.number())` - Unix timestamp of last auto-adjustment
   - [ ] `adjustment_reason: v.optional(v.string())` - Why adjustment occurred (audit trail)

9. **Backfill Existing Records**
   - [ ] Migration: Copy current `max_tokens` to `baseline_max_tokens` for all existing prompts
   - [ ] Set `adjusted_at: null`, `adjustment_reason: null` (not yet adjusted)

### UI - Visual Diff Display in AI Prompt Management

10. **Baseline vs Adjusted Badge Display**
    - [ ] Green checkmark badge: `max_tokens === baseline_max_tokens` (using baseline)
    - [ ] Blue badge with ↑ arrow: `max_tokens > baseline_max_tokens` (adjusted)
    - [ ] Badge shows diff: `"+{diff} tokens"` (e.g., "+1000")
    - [ ] Tooltip on hover: Full adjustment details (timestamp, attempts, reason)

11. **"Reset to Baseline" Admin Action**
    - [ ] Button visible only when `max_tokens > baseline_max_tokens`
    - [ ] On click: Confirm dialog → Reset `max_tokens` to `baseline_max_tokens`
    - [ ] Clear `adjusted_at` and `adjustment_reason` fields
    - [ ] Log reset action for audit trail

### Safety Mechanisms

12. **Token Escalation Limits and Warnings**
    - [ ] Environment variable: `MAX_TOKEN_ESCALATION_CAP` (default: 10000)
    - [ ] Hard cap enforcement: Cannot exceed 10,000 tokens (configurable)
    - [ ] Warning threshold: Alert admins when prompt reaches 80% of cap (8,000 tokens)
    - [ ] Cap-reached logging: Log when escalation fails due to cap limit

13. **Admin Alert UI for High Token Usage**
    - [ ] Dashboard widget: "Prompts Near Token Limit" (shows prompts at >80% of cap)
    - [ ] Highlight prompts with `max_tokens > 8000` for review
    - [ ] Link to prompt edit page for manual optimization

### Logging & Monitoring

14. **Console Logging for Debugging** (Phase 1 - No database storage yet)
    - [ ] Log all truncation detection events with correlation ID
    - [ ] Log each escalation attempt and final outcome
    - [ ] Log database updates with before/after `max_tokens` values
    - [ ] Include prompt name, operation, and timestamp in all logs

### Testing Requirements

15. **Backend Unit Tests**
    - [ ] Test `finishReason` extraction from OpenRouter response
    - [ ] Test `retryWithAdaptiveTokens` escalation logic (3-4 scenarios)
    - [ ] Test database update mutation with all parameters
    - [ ] Test token cap enforcement (escalation stops at 10,000)

16. **Backend Integration Tests**
    - [ ] Test end-to-end flow: truncation → escalation → success → database update
    - [ ] Test multiple truncations: baseline → +500 → +500 → success
    - [ ] Test cap reached: baseline → escalations fail at 10,000 limit
    - [ ] Test normal completion: no escalation when `finish_reason === "stop"`

17. **UI Component Tests**
    - [ ] Test badge display logic (baseline vs adjusted)
    - [ ] Test tooltip content rendering (adjustment details)
    - [ ] Test "Reset to Baseline" button functionality
    - [ ] Test admin alert widget (prompts near cap)

### Documentation

18. **Pattern Documentation**
    - [ ] Document "Adaptive Retry with Token Escalation" pattern in `docs/patterns/ai-service-patterns.md`
    - [ ] Include code examples, escalation logic, and safety mechanisms

19. **Implementation Guide**
    - [ ] Document new database fields and migration in `docs/architecture/database-schema.md`
    - [ ] Document environment variables (`MAX_TOKEN_ESCALATION_CAP`) in deployment guide

20. **Cleanup**
    - [ ] **Delete** `docs/investigations/step-5-end-event-question-generation-failure.md` (investigation complete, solution implemented)

## Estimation & Planning

### Story Points

**8**

### Estimated Complexity

**High**

### Estimated Time

**2-3 days**

### Risk Level

**Medium**

**Risk Factors:**
- OpenRouter API response structure changes could break `finish_reason` extraction
- Schema migration for existing prompts requires careful backfilling
- Adaptive retry logic complexity (multiple edge cases)
- UI integration with existing prompt management interface

**Mitigation Strategies:**
- Comprehensive backend tests for all escalation scenarios
- Database migration with rollback plan
- Gradual rollout: Enable for one prompt first, monitor, then expand
- Fallback pattern: If `finish_reason` not available, use existing error-based detection

## Tasks / Subtasks

### Phase 1: Backend - Truncation Detection (aiService.ts)

- [x] **Task 1: Extract `finish_reason` from OpenRouter API (AC: 1, 2)**
  - [x] Modify `callOpenRouter` function to extract `data.choices[0].finish_reason`
  - [x] Add `finishReason` field to return type (update TypeScript interface)
  - [x] Return `finishReason` alongside existing `content` and `usage` fields
  - [x] Add unit test: Verify `finishReason` extraction for all values (`"stop"`, `"length"`, `"content_filter"`)
  - [x] Add logging: Log `finishReason` value with correlation ID for debugging

### Phase 2: Backend - Adaptive Retry Logic (aiClarification.ts)

- [x] **Task 2: Create `retryWithAdaptiveTokens` Function (AC: 3, 4, 5)**
  - [x] Create new function signature: `retryWithAdaptiveTokens<T>(operation, context, escalation_increment, max_escalations)`
  - [x] Implement escalation loop: Start with `baseline_max_tokens`, add 500 per retry
  - [x] Add truncation detection: Check `finishReason === "length"` OR JSON parse errors
  - [x] Add early exit: Stop on `finishReason === "stop"` (normal completion)
  - [x] Enforce token cap: Read `MAX_TOKEN_ESCALATION_CAP` environment variable (default 10000)
  - [x] Add comprehensive logging: Log each attempt with max_tokens, increment, attempt number

- [x] **Task 3: Replace `retryWithBackoff` with `retryWithAdaptiveTokens` (AC: 3)**
  - [x] Update all question generation callsites to use new retry function
  - [x] Pass `prompt_name` and `baseline_max_tokens` in context parameter
  - [x] Update error messages to include escalation details
  - [x] Add unit tests:
    - Baseline success (no escalation needed)
    - Single escalation (+500) then success
    - Multiple escalations (+500, +1000) then success
    - Cap reached (fail after max escalations)

### Phase 3: Backend - Self-Healing Database Updates (promptManager.ts, schema.ts)

- [x] **Task 4: Add New Schema Fields to `ai_prompts` Table (AC: 8, 9)**
  - [x] Add `baseline_max_tokens: v.optional(v.number())` to schema.ts
  - [x] Add `adjusted_at: v.optional(v.number())` to schema.ts
  - [x] Add `adjustment_reason: v.optional(v.string())` to schema.ts
  - [x] Create migration: Backfill `baseline_max_tokens` with current `max_tokens` values
  - [x] Test migration locally: Verify all existing prompts have `baseline_max_tokens` populated

- [x] **Task 5: Create `updatePromptTokenLimit` Mutation (AC: 6, 7)**
  - [x] Create mutation in promptManager.ts
  - [x] Accept parameters: `prompt_name`, `new_max_tokens`, `baseline_max_tokens`, `adjustment_reason`
  - [x] Update `ai_prompts` record:
    - Set `max_tokens = new_max_tokens`
    - Set `baseline_max_tokens` (if not already set)
    - Set `adjusted_at = Date.now()`
    - Set `adjustment_reason` (auto-generated or manual)
  - [x] Add audit logging with correlation ID
  - [x] Add unit tests: Verify all fields updated correctly

- [x] **Task 6: Integrate Database Update in Retry Logic (AC: 7)** ✅ **COMPLETED**
  - [x] In `retryWithAdaptiveTokens`: On successful escalation, call `updatePromptTokenLimit`
  - [x] Pass: `prompt_name`, final `max_tokens`, original `baseline_max_tokens`
  - [x] Generate `adjustment_reason`: `"Auto-escalated: {count} truncation(s) detected ({type})"`
  - [x] Make database update non-blocking (try-catch, log errors, don't fail main operation)
  - [x] Add integration test: Verify database update after escalation

### Phase 4: Frontend - UI Visual Diff Display

- [ ] **Task 7: Add Badge Display Logic to AI Prompt Management UI (AC: 10)**
  - [ ] Locate or create AI Prompt Management component (`apps/web/components/admin/ai-prompts/`)
  - [ ] Add badge rendering logic:
    - Green checkmark: `max_tokens === baseline_max_tokens`
    - Blue ↑ badge: `max_tokens > baseline_max_tokens` with `"+{diff}"` text
  - [ ] Add tooltip component: Display `adjustment_reason`, `adjusted_at` timestamp, diff amount
  - [ ] Style with Tailwind CSS and ShadCN UI components
  - [ ] Add component tests: Verify badge logic for baseline, adjusted, and null values

- [ ] **Task 8: Add "Reset to Baseline" Admin Action (AC: 11)**
  - [ ] Add "Reset to Baseline" button (visible only when `max_tokens > baseline_max_tokens`)
  - [ ] Implement confirm dialog before reset
  - [ ] Create mutation: `resetPromptToBaseline(prompt_name)`
  - [ ] Reset logic: Set `max_tokens = baseline_max_tokens`, clear `adjusted_at`, clear `adjustment_reason`
  - [ ] Add audit log entry for reset action
  - [ ] Add component tests: Verify button visibility and reset behavior

- [ ] **Task 9: Add Admin Alert Widget for High Token Usage (AC: 12, 13)**
  - [ ] Create "Prompts Near Token Limit" dashboard widget
  - [ ] Query: Find prompts where `max_tokens > (MAX_TOKEN_ESCALATION_CAP * 0.8)` (default >8000)
  - [ ] Display: List of prompts with current `max_tokens`, baseline, diff
  - [ ] Link to prompt edit page for manual review/optimization
  - [ ] Add styling: Yellow warning badge for prompts near cap

### Phase 5: Safety, Logging, & Testing

- [ ] **Task 10: Implement Safety Mechanisms (AC: 12, 13)**
  - [ ] Add environment variable: `MAX_TOKEN_ESCALATION_CAP` (default 10000)
  - [ ] Enforce cap in `retryWithAdaptiveTokens`: Throw clear error when cap reached
  - [ ] Add warning threshold logging: Log when prompt exceeds 80% of cap
  - [ ] Add admin alert: Email/Slack notification when prompt hits 80% threshold

- [ ] **Task 11: Add Comprehensive Logging (AC: 14)**
  - [ ] Console logging for all truncation detection events
  - [ ] Console logging for each escalation attempt (max_tokens, attempt number)
  - [ ] Console logging for successful database updates (before/after values)
  - [ ] Include correlation ID in all log entries

- [ ] **Task 12: Write Comprehensive Tests (AC: 15, 16, 17)**
  - [ ] Backend unit tests: `finishReason` extraction, retry logic, mutation
  - [ ] Backend integration tests: End-to-end flows (truncation → escalation → update)
  - [ ] UI component tests: Badge display, tooltip, reset button, admin widget
  - [ ] Run `bun run typecheck`, `bun run lint`, `bun test` - All must pass

### Phase 6: Documentation & Cleanup

- [ ] **Task 13: Document Patterns and Implementation (AC: 18, 19)**
  - [ ] Create/update `docs/patterns/ai-service-patterns.md` with "Adaptive Retry with Token Escalation" pattern
  - [ ] Include: Code examples, escalation logic diagram, safety mechanisms
  - [ ] Update `docs/architecture/database-schema.md` with new `ai_prompts` fields
  - [ ] Document environment variables in deployment guide

- [ ] **Task 14: Cleanup Investigation Files (AC: 20)**
  - [ ] Delete `docs/investigations/step-5-end-event-question-generation-failure.md`
  - [ ] Verify file no longer referenced in any documentation

- [ ] **Task 15: CI Verification (NON-NEGOTIABLE)**
  - [ ] Run local verification suite: `bun run typecheck`, `bun run lint`, `bun test`, `bun run build`
  - [ ] Push changes and monitor CI: `bun run ci:status`
  - [ ] Verify CI SUCCESS before marking story complete
  - [ ] If CI fails: Fix issues immediately before proceeding

## Documentation Impact Assessment

**KDD - Knowledge-Driven Development Considerations:**

### Architectural Patterns to Document

1. **Adaptive Retry with Token Escalation Pattern** (NEW)
   - Self-healing AI service pattern that automatically adjusts token limits
   - Progressive escalation strategy (baseline → +500 increments)
   - Database persistence of learned optimal values
   - Safety mechanisms (caps, warnings, admin alerts)
   - **File**: `docs/patterns/ai-service-patterns.md` (create or extend)

2. **Database Schema Evolution Pattern** (EXISTING)
   - Adding optional fields with backfill migration
   - Preserving baseline values alongside adjusted values
   - Audit trail fields (adjusted_at, adjustment_reason)
   - **File**: `docs/patterns/database-patterns.md` (may need updates)

3. **UI Visual Diff Display Pattern** (NEW)
   - Color-coded badges for baseline vs adjusted values
   - Tooltips with adjustment context and timestamp
   - Admin reset actions with confirmation dialogs
   - **File**: `docs/patterns/frontend-patterns.md` (extend)

### Documentation Files to Update

1. **`docs/architecture/database-schema.md`** (if exists) OR **`docs/architecture/data-models.md`**
   - Add new `ai_prompts` table fields documentation
   - Document backfill migration strategy
   - Include example schema with new fields

2. **`docs/technical-guides/environment-variables.md`** (if exists)
   - Document `MAX_TOKEN_ESCALATION_CAP` environment variable
   - Default value, purpose, and when to adjust

3. **`docs/technical-guides/ai-service-integration.md`** (if exists)
   - Document OpenRouter `finish_reason` field and values
   - Truncation detection patterns
   - Adaptive retry best practices

### Examples to Create

1. **Example: Adaptive Retry Implementation** (`docs/examples/ai-service/adaptive-retry/`)
   - Simplified code example showing retry escalation
   - Mock OpenRouter responses for different scenarios
   - Example logging output for debugging

2. **Example: Database Migration with Backfill** (`docs/examples/database/schema-backfill/`)
   - Example migration code for adding optional fields
   - Backfill strategy for existing records
   - Testing approach for migrations

3. **Example: UI Badge Components** (`docs/examples/ui-components/status-badges/`)
   - Reusable badge component patterns
   - Tooltip integration examples
   - Color-coded diff displays

### New Knowledge to Capture

1. **OpenRouter API Behavior**
   - `finish_reason` field values and meanings
   - When truncation occurs vs normal completion
   - How to detect truncation reliably

2. **Token Escalation Heuristics**
   - Why 500 token increments? (user preference, could be 200)
   - Why 10,000 token cap? (cost vs completeness tradeoff)
   - Why 80% warning threshold? (early warning before hitting cap)

3. **Self-Healing System Design**
   - Database as learned configuration storage
   - Baseline preservation for reset capability
   - Automatic vs manual adjustment workflows

### KDD Process for This Story

After implementation completion:

1. **Extract Patterns**: Run `*task capture-kdd-knowledge` to extract patterns to `docs/patterns/`
2. **Create Examples**: Extract working code to `docs/examples/` for reuse
3. **Document Lessons**: Capture insights to `docs/lessons-learned/story-6-9-adaptive-tokens-kdd.md`
4. **Update Architecture**: Revise architecture docs with new schema and patterns

**CRITICAL**: Story file should contain only **LINKS** to knowledge base files, NOT detailed KDD content. All patterns, examples, and lessons go to permanent knowledge base files for future development.

## Dev Notes

### Relevant Source Tree Information

**[Source: docs/architecture/source-tree.md]**

Project structure for Story 6.9 implementation:

```
apps/
├── convex/                          # Backend Application
│   ├── aiService.ts                 # MODIFY: Extract finish_reason field
│   ├── aiClarification.ts           # MODIFY: Replace retryWithBackoff with retryWithAdaptiveTokens
│   ├── promptManager.ts             # MODIFY: Add updatePromptTokenLimit mutation
│   ├── schema.ts                    # MODIFY: Add baseline_max_tokens, adjusted_at, adjustment_reason fields
│   └── migrations.ts                # ADD: Backfill migration for existing prompts
│
├── web/                             # Frontend Application
│   ├── components/
│   │   └── admin/
│   │       └── ai-prompts/          # MODIFY OR CREATE: AI Prompt Management UI
│   │           ├── prompt-list.tsx  # Badge display logic
│   │           ├── prompt-card.tsx  # Visual diff badges and tooltips
│   │           └── admin-dashboard.tsx # Token limit alert widget
│   └── lib/
│       └── convex-api.ts            # MAY NEED: Type definitions for new mutation

tests/
├── convex/
│   ├── src/
│   │   ├── aiService.test.ts        # ADD: finishReason extraction tests
│   │   ├── aiClarification.test.ts  # ADD: Adaptive retry logic tests
│   │   └── promptManager.test.ts    # ADD: updatePromptTokenLimit mutation tests
│   └── integration/
│       └── ai-token-escalation.test.ts # ADD: End-to-end flow tests

docs/
├── patterns/
│   └── ai-service-patterns.md       # CREATE OR EXTEND: Adaptive retry pattern
├── examples/
│   ├── ai-service/
│   │   └── adaptive-retry/          # CREATE: Example implementation
│   └── database/
│       └── schema-backfill/         # CREATE: Migration example
└── investigations/
    └── step-5-end-event-question-generation-failure.md # DELETE after story complete
```

### Architecture Context

**[Source: docs/architecture/tech-stack.md]**

**Core Technologies:**
- **Convex 1.12.x**: Backend platform for mutations, queries, database
- **TypeScript 5.4.x**: Strict mode enabled, no-any policy
- **Zod 3.23.x**: Schema validation (used in Convex function inputs)
- **Next.js 14.2.x**: Frontend application
- **Bun 1.1.x**: Package manager and test runner

**[Source: docs/architecture/coding-standards.md]**

**Critical Coding Standards:**
- **No `any` Type Policy**: Use `@ts-expect-error` with explanation for unavoidable issues (e.g., Convex type inference)
- **No Direct `process.env`**: Use centralized configuration management
- **Function Component Style**: Use `export function ComponentName()` NOT arrow functions
- **Environment-Aware Patterns**: Defensive configuration with meaningful error messages

**[Source: docs/architecture/error-handling-strategy.md]**

**Production-Safe Error Handling Pattern:**
```typescript
// CORRECT: Works in both dev and production
catch (error) {
  // Read error.data first (production), fallback to error.message (dev)
  const errorText = error.data || error.message || 'An error occurred';

  if (errorText.includes('specific error pattern')) {
    return { success: false, error: 'User-friendly message' };
  }

  return { success: false, error: errorText };
}
```

**ConvexError Backend Pattern:**
```typescript
// Backend: Proper ConvexError with try-catch wrapper
export const mutationName = mutation({
  handler: async (ctx, args) => {
    try {
      // ... operation logic
      if (!condition) {
        throw new ConvexError('Specific error message');
      }
      return result;
    } catch (error) {
      if (error instanceof ConvexError) {
        throw error; // Re-throw to preserve error.data
      }
      throw new ConvexError(`Operation failed: ${error.message}`);
    }
  }
});
```

**[Source: apps/convex/schema.ts lines 418-458]**

**Current `ai_prompts` Table Schema:**
```typescript
ai_prompts: defineTable({
  // Prompt Identity
  prompt_name: v.string(),         // e.g., "generate_clarification_questions"
  prompt_version: v.string(),      // e.g., "v1.2.0"

  // Prompt Content
  prompt_template: v.string(),
  description: v.optional(v.string()),

  // AI Configuration
  ai_model: v.string(),            // REQUIRED: e.g., "openai/gpt-5"
  max_tokens: v.optional(v.number()), // Current effective limit (e.g., 2000)
  temperature: v.optional(v.number()),

  // ✅ ADD THESE NEW FIELDS:
  // baseline_max_tokens: v.optional(v.number()), // Original coded value (e.g., 2000)
  // adjusted_at: v.optional(v.number()),         // Unix timestamp of adjustment
  // adjustment_reason: v.optional(v.string()),   // Why adjustment occurred

  // Versioning
  is_active: v.optional(v.boolean()),
  created_at: v.number(),
  created_by: v.optional(v.id("users")),

  // Performance Metrics
  usage_count: v.optional(v.number()),
  average_response_time: v.optional(v.number()),
  success_rate: v.optional(v.number()),
})
  .index("by_name", ["prompt_name"])
  .index("by_name_version", ["prompt_name", "prompt_version"])
  .index("by_active", ["is_active"])
  .index("by_workflow", ["workflow_step"])
  .index("by_subsystem", ["subsystem"]),
```

**[Source: apps/convex/schema.ts lines 461-495]**

**Current `ai_request_logs` Table Schema:**
```typescript
ai_request_logs: defineTable({
  // Request Identification
  correlation_id: v.string(),
  operation: v.string(),

  // AI Service Details
  model: v.string(),
  prompt_template: v.string(),

  // Performance Metrics
  processing_time_ms: v.number(),
  tokens_used: v.optional(v.number()),  // ✅ Already tracks token usage!
  cost_usd: v.optional(v.number()),

  // Status & Error Handling
  success: v.boolean(),
  error_message: v.optional(v.string()),

  // Context
  user_id: v.optional(v.id("users")),
  incident_id: v.optional(v.id("incidents")),

  // Timestamps
  created_at: v.number(),
})
```

**Note**: `ai_request_logs` table exists but is currently **empty** in both dev and production. No historical token usage data available yet. Analytics features deferred to Story 6.10+ after accumulating data.

**[Source: apps/convex/promptManager.ts investigation]**

**Seeding Fallback Pattern (Current Behavior):**
```typescript
// Lines 197, 246 in seedPromptTemplates mutation
max_tokens: (defaultPrompt as any).max_tokens || 2000,  // ← Hardcoded fallback
```

**Why prompts have 2000 in database**: Template definitions (lines 664-706) don't include `max_tokens` field, so seeding uses 2000 as fallback. This explains current END_EVENT truncation issue.

**[Source: apps/convex/aiService.ts investigation]**

**Current OpenRouter API Call (Missing `finish_reason`):**
```typescript
// Lines 155-164 - Current implementation
const data = await response.json();

if (!data.choices || !data.choices[0] || !data.choices[0].message) {
  throw new Error('Invalid response format from OpenRouter API');
}

return {
  content: data.choices[0].message.content,
  usage: data.usage,
  // ❌ MISSING: finishReason extraction
};
```

**Required Change:**
```typescript
return {
  content: data.choices[0].message.content,
  usage: data.usage,
  finishReason: data.choices[0].finish_reason,  // ✅ ADD THIS
};
```

**[Source: apps/convex/aiClarification.ts investigation]**

**Current Retry Logic (To Be Replaced):**
```typescript
// Lines 10-57 - Current retryWithBackoff function
const RETRY_CONFIG = {
  MAX_ATTEMPTS: 3,
  BASE_DELAY: 1000,
  MAX_DELAY: 8000,
  BACKOFF_MULTIPLIER: 2
};

// Problem: Retries with SAME max_tokens value
// Result: Same truncation 3 times, ~3 seconds wasted
```

**Replacement Strategy**: `retryWithAdaptiveTokens` function with progressive escalation (baseline → +500 → +500 → +500).

**[Source: docs/prd/epic-6.md lines 325-474]**

**Epic 6.9 - Production Evidence:**
- **Bug**: END_EVENT question generation shows spinner indefinitely
- **Root Cause**: `max_tokens: 2000` insufficient for gpt-5 detailed responses
- **Evidence**: All question generation prompts have 2000 in database, no `max_tokens` in code
- **Impact**: 6-key-area prompt (longest) → highest token usage → exceeds 2000 → truncation → JSON parse failure → silent failure

**Self-Healing Design Goals:**
- Detect truncation automatically (via `finish_reason === "length"`)
- Retry with increased tokens (progressive escalation)
- Persist learned optimal value to database
- Visual UI indication of baseline vs adjusted values
- Admin controls for reset and monitoring

### Pattern Validation

**[Source: docs/patterns/frontend-patterns.md, docs/patterns/backend-patterns.md - References]**

**Patterns to Follow:**

1. **Repository Pattern for Data Access** (Existing)
   - All database mutations go through Convex mutations
   - No direct database access from frontend
   - Type-safe interfaces with Zod validation

2. **Error Handling Pattern** (Existing - CRITICAL for production)
   - Use `error.data || error.message` pattern for ConvexError
   - Backend re-throws ConvexError to preserve error.data
   - Frontend reads error.data first (production), falls back to error.message (dev)

3. **Environment Configuration Pattern** (Existing)
   - No direct `process.env` access
   - Centralized configuration with defaults
   - Defensive validation with meaningful errors

4. **Adaptive Retry Pattern** (NEW - To Be Documented)
   - Progressive escalation strategy
   - Detection → Escalation → Success → Persistence
   - Safety limits and admin controls

5. **Database Schema Evolution Pattern** (Existing)
   - Optional fields for backward compatibility
   - Backfill migrations for existing records
   - Audit trail fields (timestamp, reason)

6. **UI Badge and Tooltip Pattern** (Existing - ShadCN UI)
   - Color-coded status badges (green checkmark, blue ↑ arrow)
   - Tooltips with detailed context on hover
   - Consistent Tailwind CSS styling

**New Patterns to Document (KDD Process):**
- Adaptive Token Escalation Pattern → `docs/patterns/ai-service-patterns.md`
- Self-Healing Database Update Pattern → `docs/patterns/backend-patterns.md`
- Visual Diff Display Pattern → `docs/patterns/frontend-patterns.md`

### Testing Standards

**[Source: docs/testing/technical/test-strategy-and-standards.md - Inferred]**

**Test File Locations:**
- **Convex Backend Tests**: `tests/convex/src/` (unit tests), `tests/convex/integration/` (integration tests)
- **Frontend Component Tests**: `tests/web/components/` (React component tests)

**Test Standards:**
- **Framework**: Jest + React Testing Library (RTL)
- **Coverage Target**: >80% for new code
- **Test Types**: Unit, Integration, E2E (Playwright for critical flows)
- **Import Aliases**: Use `@/` aliases instead of relative paths (e.g., `import { mutation } from '@/convex'`)
- **Pragmatic TypeScript**: Use `@ts-nocheck` for test interface issues that don't affect functionality

**Testing Requirements for Story 6.9:**

1. **Backend Unit Tests** (`tests/convex/src/`):
   - `aiService.test.ts`: Test `finish_reason` extraction for all values (`"stop"`, `"length"`, `"content_filter"`)
   - `aiClarification.test.ts`: Test `retryWithAdaptiveTokens` escalation logic (baseline, +500, +1000, cap enforcement)
   - `promptManager.test.ts`: Test `updatePromptTokenLimit` mutation with all parameters

2. **Backend Integration Tests** (`tests/convex/integration/`):
   - `ai-token-escalation.test.ts`: End-to-end flow (truncation → escalation → success → database update)
   - Test scenarios: Single escalation, multiple escalations, cap reached, normal completion

3. **UI Component Tests** (`tests/web/components/`):
   - Badge display logic (baseline vs adjusted)
   - Tooltip rendering (adjustment details)
   - "Reset to Baseline" button functionality
   - Admin alert widget (prompts near cap)

4. **CI Verification** (NON-NEGOTIABLE):
   - All tests must pass: `bun test`
   - TypeScript compilation: `bun run typecheck`
   - Linting: `bun run lint`
   - Production build: `bun run build`
   - CI pipeline: `bun run ci:status` (must show SUCCESS)

**Test Execution Commands:**
```bash
# Run all tests with coverage and watch mode
bun test:convex:coverage:watch:all    # Backend tests
bun test:web:coverage:watch:all       # Frontend tests

# Run all tests once (CI mode)
bun test

# CI verification suite
bun run typecheck && bun run lint && bun test && bun run build && bun run ci:status
```

**Testing Philosophy** (Story 4.2 Lesson):
- **Pragmatic vs Perfectionist**: Test behavior and outcomes, not implementation details
- Use `toBeGreaterThan()`, `toBeLessThan()` for non-critical numeric values
- Verify expectations against actual function output before writing assertions
- Document precise assertions with comments explaining why

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-27 | 1.0 | Initial story draft created by Bob the Scrum Master | Bob (BMAD Scrum Master Agent) |

## Dev Agent Record

### Agent Model Used

**Model**: Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)
**Agent Persona**: James (Dev Agent)
**Date**: 2025-10-27

### Debug Log References

*(To be populated as implementation progresses)*

### Completion Notes List

#### Phase 1: Task 1 - Extract `finish_reason` from OpenRouter API ✅ COMPLETE

**Implementation Details**:
- Modified `AIResponse` interface to include optional `finishReason?: string` field (apps/convex/aiService.ts:43)
- Updated `makeAPICall` method to extract `data.choices[0].finish_reason` from OpenRouter response (apps/convex/aiService.ts:166)
- Updated `sendRequest` method to pass through `finishReason` in return object (apps/convex/aiService.ts:102)
- Added console logging for `finishReason` with correlation ID for debugging (apps/convex/aiService.ts:90-92)

**Testing**:
- Created comprehensive unit tests in `tests/convex/src/aiService.test.ts` (234 lines)
- 6 test cases covering all `finish_reason` values:
  - `"stop"` (normal completion)
  - `"length"` (truncated response)
  - `"content_filter"` (policy violation)
  - `undefined` (missing field - graceful handling)
  - Successful response structure validation
  - Failed response structure validation

**Validation Results**:
- ✅ TypeScript typecheck: PASSED
- ✅ All 6 unit tests: PASSED
- ⚠️ Lint: 112 pre-existing errors (none from Story 6.9 changes)

#### Phase 2: Task 2 - Create `retryWithAdaptiveTokens` Function ✅ COMPLETE

**Implementation Details**:
- Created new `retryWithAdaptiveTokens` generic function with comprehensive token escalation logic (apps/convex/aiClarification.ts:79-246)
- Added `TOKEN_ESCALATION_CONFIG` constants for default values (apps/convex/aiClarification.ts:17-22)
- Function signature: `retryWithAdaptiveTokens<T>(operation, context, escalation_increment, max_escalations)`
- Escalation pattern: baseline → +500 → +500 → +500 (configurable)
- Truncation detection via `finishReason === "length"` OR JSON parse errors
- Early exit on `finishReason === "stop"` (normal completion)
- Token cap enforcement from `MAX_TOKEN_ESCALATION_CAP` environment variable (default: 10000)
- Warning threshold at 80% of cap (8000 tokens)

**Features Implemented**:
1. **Adaptive Escalation Loop**: Starts with `baseline_max_tokens`, adds increment on each retry
2. **Truncation Detection**:
   - Primary: `finishReason === "length"` from OpenRouter API
   - Fallback: JSON parse errors (potential truncation indicator)
3. **Smart Exit Strategy**:
   - Success on `finishReason === "stop"` or other non-length values
   - Immediate throw on non-truncation errors (auth, network, etc.)
4. **Safety Mechanisms**:
   - Token cap enforcement (default 10,000 tokens)
   - Clear error messages with troubleshooting guidance
   - Maximum escalation limit (default: 3 attempts)
5. **Comprehensive Logging**:
   - Start log: baseline, increment, max escalations, token cap, correlation ID
   - Attempt log: current tokens, increment from baseline, attempt number
   - Truncation detection log: finish_reason or JSON parse error
   - Success log: final tokens, total escalation, escalations used
   - Error logs: token cap reached, non-truncation errors, all escalations exhausted

**Return Structure**:
```typescript
{
  result: T;                    // The successful operation result
  final_max_tokens: number;     // Final token value used for success
  escalations_used: number;     // Number of escalations performed (0 = baseline success)
  finishReason?: string;        // OpenRouter finish_reason value
}
```

**Validation Results**:
- ✅ TypeScript typecheck: PASSED
- ✅ Function compiles without errors
- ⏳ Unit tests: Will be added in Task 3 alongside integration

#### Phase 2: Task 3 - Integrate `retryWithAdaptiveTokens` into Question Generation ✅ COMPLETE

**Implementation Details**:
- Exported `retryWithAdaptiveTokens` from aiClarification.ts (line 79)
- Imported adaptive retry function in questionGenerator.ts (line 11)
- Refactored `generateQuestionsWithTemplate` to use adaptive token management (apps/convex/lib/ai/questionGenerator.ts:122-238)
- Wrapped AI call in adaptive retry logic with dynamic maxTokens parameter
- Added escalation result logging (final_max_tokens, escalations_used, finish_reason)

**Integration Pattern**:
1. **Extract baseline_max_tokens** from prompt template (line 123)
2. **Create AI operation function** that accepts dynamic maxTokens (lines 134-209):
   - Calls `aiManager.sendRequest` with current maxTokens
   - Logs AI response with finishReason
   - Parses JSON response (throws on parse errors for escalation)
   - Returns `{ result, finishReason }` structure
3. **Execute with adaptive retry** (lines 212-219):
   - Passes operation, context (prompt_name, baseline_max_tokens, correlation_id)
   - Automatically escalates on `finishReason === "length"` or JSON parse errors
   - Returns escalation result with final tokens and escalation count
4. **Log escalation outcome** (lines 221-228)

**Error Handling**:
- JSON parse errors trigger token escalation (treated as potential truncation)
- Non-truncation errors (auth, network) throw immediately without escalation
- Clear error messages include escalation details and troubleshooting guidance

**Testing**:
- Created comprehensive test suite: `tests/convex/src/retryWithAdaptiveTokens.test.ts` (11 test cases, all passing)
- Test scenarios:
  - ✅ Baseline success (no escalation) - 1 test
  - ✅ Single escalation (+500) - 2 tests (finishReason + JSON parse error)
  - ✅ Multiple escalations (+500, +1000) - 1 test
  - ✅ Maximum escalations reached - 1 test
  - ✅ Token cap enforcement - 2 tests (default + custom)
  - ✅ Non-truncation errors (immediate throw) - 2 tests
  - ✅ Early exit on normal completion - 2 tests (stop + content_filter)

**Validation Results**:
- ✅ TypeScript typecheck: PASSED
- ✅ All 11 unit tests: PASSED (0.222s)
- ✅ Integration complete: All question generation now uses adaptive token management

#### Phase 3: Task 4 - Add New Schema Fields to `ai_prompts` Table ✅ COMPLETE

**Implementation Details**:
- Added three new optional fields to `ai_prompts` table schema (apps/convex/schema.ts:442-445):
  - `baseline_max_tokens: v.optional(v.number())` - Original max_tokens before adjustments
  - `adjusted_at: v.optional(v.number())` - Timestamp when max_tokens was last adjusted
  - `adjustment_reason: v.optional(v.string())` - Reason for adjustment (e.g., "Auto-escalated: 3 truncations")
- Created migration function `backfillAIPromptBaselineTokens` (apps/convex/migrations.ts:385-465)
- Migration logic:
  1. Query all ai_prompts records
  2. Skip records that already have baseline_max_tokens
  3. Skip records that have no max_tokens to copy
  4. Set `baseline_max_tokens = current max_tokens` for remaining records
  5. Return detailed summary with updated/skipped/error counts

**Migration Results**:
- ✅ Total prompts found: 126
- ✅ Successfully backfilled: 126 prompts
- ✅ Skipped: 0 (all records needed backfill)
- ✅ Errors: 0 (no failures)
- ✅ Verification: Confirmed `baseline_max_tokens` field populated via data query
- Sample values:
  - `generate_clarification_questions_*`: baseline_max_tokens = 2000
  - `enhance_narrative_*`: baseline_max_tokens = 4000

**Database Verification**:
```bash
bunx convex data ai_prompts --limit 3
# Output shows baseline_max_tokens: 2000 (matches max_tokens)
```

**Validation Results**:
- ✅ TypeScript typecheck: PASSED (10.181s)
- ✅ Schema fields added successfully
- ✅ Migration executed successfully
- ✅ Data verification complete

#### Phase 3: Task 5 - Create `updatePromptTokenLimit` Mutation ✅ COMPLETE

**Implementation Details**:
- Created mutation in promptManager.ts (apps/convex/promptManager.ts:1159-1242)
- Mutation signature: `updatePromptTokenLimit(prompt_name, new_max_tokens, baseline_max_tokens, adjustment_reason, correlation_id)`
- Update logic:
  1. Find active prompt by name
  2. Set `max_tokens = new_max_tokens`
  3. Set `baseline_max_tokens` (backfill if not set, preserve if already set, override if explicitly provided)
  4. Set `adjusted_at = Date.now()`
  5. Set `adjustment_reason` (from parameter)
  6. Return comprehensive result with old/new values and correlation ID

**Key Features**:
- **Smart Baseline Handling**:
  - If baseline_max_tokens already exists → preserve it (don't overwrite)
  - If baseline_max_tokens not set → backfill with current max_tokens
  - If baseline_max_tokens explicitly provided → use provided value
- **Comprehensive Logging**:
  - Start log: prompt_name, new_max_tokens, baseline, adjustment_reason, correlationId
  - Success log: prompt_id, old/new max_tokens, baseline, adjustment_reason, correlationId
  - Error log: prompt not found with correlationId
- **Correlation ID Tracking**:
  - Accepts optional correlation_id parameter for request tracing
  - Auto-generates ID if not provided: `token-update-${timestamp}`

**Testing**:
- Created comprehensive test suite: `tests/convex/src/updatePromptTokenLimit.test.ts` (10 test cases, all passing)
- Test scenarios:
  - ✅ Update max_tokens and set baseline when not previously set
  - ✅ Update max_tokens without changing existing baseline
  - ✅ Explicitly set baseline when provided as argument
  - ✅ Include correlation_id in logging when provided
  - ✅ Generate correlation_id when not provided
  - ✅ Throw error when prompt not found
  - ✅ Throw error when prompt is inactive
  - ✅ Track single escalation reason
  - ✅ Track multiple escalation reason
  - ✅ Allow manual adjustment reasons

**Return Structure**:
```typescript
{
  success: true,
  prompt_id: string,
  prompt_name: string,
  old_max_tokens: number,
  new_max_tokens: number,
  baseline_max_tokens: number,
  adjusted_at: number,
  adjustment_reason: string,
  correlationId: string,
}
```

**Validation Results**:
- ✅ TypeScript typecheck: PASSED (10.35s)
- ✅ All 10 unit tests: PASSED (0.157s)
- ✅ Mutation compiles without errors
- ✅ Smart baseline preservation logic verified

---

## Task 6 Completion Notes: Database Update Integration

**Date**: 2025-10-28
**Duration**: Implementation + testing
**Status**: ✅ COMPLETED

### Implementation Summary

Integrated database persistence into the `retryWithAdaptiveTokens` function to automatically save token adjustments when escalation occurs.

**Key Changes**:
1. **Modified Function Signature**: Added `ctx` parameter to enable mutation calls
2. **Truncation Type Tracking**: Added `truncationType` variable to track escalation cause
3. **Non-Blocking Database Update**: Wrapped mutation call in try-catch to prevent failures
4. **Smart Adjustment Reason**: Auto-generates descriptive reasons based on truncation type

**Database Update Logic**:
```typescript
// Only update if escalation occurred (attempt > 1)
if (attempt > 1) {
  try {
    const escalationCount = attempt - 1;
    const truncationCause = truncationType || 'Unknown truncation';
    const adjustmentReason = `Auto-escalated: ${escalationCount} truncation(s) detected (${truncationCause})`;

    await ctx.runMutation(api.promptManager.updatePromptTokenLimit, {
      prompt_name: context.prompt_name,
      new_max_tokens: currentMaxTokens,
      adjustment_reason: adjustmentReason,
      correlation_id: context.correlation_id,
    });
  } catch (dbError) {
    // Non-blocking: log error but don't fail main operation
    console.warn(`⚠️ DATABASE UPDATE FAILED (non-blocking)`, { ... });
  }
}
```

**Files Modified**:
- `apps/convex/aiClarification.ts` - Added ctx parameter, truncationType tracking, database update logic
- `apps/convex/lib/ai/questionGenerator.ts` - Updated retryWithAdaptiveTokens call to pass ctx
- `tests/convex/src/retryWithAdaptiveTokens.test.ts` - Updated all tests + added 5 integration tests

**Test Results**:
- ✅ All 16 existing tests: PASSED (adapted for new ctx parameter)
- ✅ 5 new integration tests: PASSED
  - Verify database update called on escalation
  - Verify database update NOT called without escalation
  - Verify non-blocking behavior on database failures
  - Verify correct adjustment_reason for finish_reason="length"
  - Verify correct adjustment_reason for JSON parse error
- ✅ TypeScript typecheck: PASSED
- ✅ Total test count: 26 tests (16 adaptive token + 10 updatePromptTokenLimit)

**Validation**:
```bash
# All tests pass
$ cd apps/convex && npx jest --testPathPattern="retryWithAdaptiveTokens" --no-coverage
PASS ../../tests/convex/src/retryWithAdaptiveTokens.test.ts (16/16 tests)

$ cd apps/convex && npx jest --testPathPattern="updatePromptTokenLimit" --no-coverage
PASS ../../tests/convex/src/updatePromptTokenLimit.test.ts (10/10 tests)

# TypeScript compiles cleanly
$ bun run typecheck
$ tsc --noEmit
✅ No errors
```

**Integration Test Highlights**:
The new "Database Update Integration" test suite verifies:
1. **Escalation triggers update**: When retryWithAdaptiveTokens escalates tokens, database is updated
2. **Baseline doesn't trigger update**: When operation succeeds with baseline tokens, no database call
3. **Non-blocking failure**: Database errors don't crash the main operation
4. **Correct reason tracking**: adjustment_reason reflects actual truncation cause (length vs JSON parse)

**Architectural Note**:
The term "integration test" in Task 6 refers to **unit tests that verify the integration point** between `retryWithAdaptiveTokens` and `updatePromptTokenLimit`. These are not end-to-end integration tests (which would use a real Convex backend), but rather unit tests with mocked dependencies that verify the function calls the mutation correctly.

### File List

**Modified Files**:
- `apps/convex/aiService.ts` - Added `finishReason` extraction, logging, and interface updates
- `apps/convex/aiClarification.ts` - Added `retryWithAdaptiveTokens` function, TOKEN_ESCALATION_CONFIG, and exported for use
- `apps/convex/lib/ai/questionGenerator.ts` - Integrated adaptive token retry into `generateQuestionsWithTemplate`
- `apps/convex/schema.ts` - Added three new fields to ai_prompts table (baseline_max_tokens, adjusted_at, adjustment_reason)
- `apps/convex/migrations.ts` - Added backfillAIPromptBaselineTokens migration function
- `apps/convex/promptManager.ts` - Added updatePromptTokenLimit mutation for adaptive token management

**Created Files**:
- `tests/convex/src/aiService.test.ts` - Unit tests for `finish_reason` extraction (6 tests)
- `tests/convex/src/retryWithAdaptiveTokens.test.ts` - Unit tests for adaptive token escalation (11 tests)
- `tests/convex/src/updatePromptTokenLimit.test.ts` - Unit tests for updatePromptTokenLimit mutation (10 tests)

## Worktree Handoff

*(Optional - only populated if story developed in worktree)*

## QA Results

*(This section will be populated by QA Agent after story implementation)*

### Pattern Compliance Review

*(To be populated by QA agent)*

### Knowledge Capture Reference

*(To be populated by QA agent - LINKS ONLY to knowledge base files, NOT detailed KDD content)*

### Velocity Data

*(To be populated by QA agent)*
